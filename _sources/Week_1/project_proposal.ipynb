{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "848dcf44",
   "metadata": {},
   "source": [
    "# ASL Video-to-Text Translation Using CNN-Based Hand Sign Recognition and LLM Sequence Reconstruction\n",
    "\n",
    "**Authors:** Usher Raymond Abainza, Dane Casey Casino, Kein Jake Culanggo, and Karylle dela Cruz\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Background\n",
    "\n",
    "American Sign Language (ASL) serves as a vital means of communication for the Deaf and Hard-of-Hearing (DHH) community. However, the language barrier between sign language users and non-signers often hinders effective communication. Translating ASL into written or spoken text can bridge this gap, especially in digital and educational environments. \n",
    "\n",
    "Traditional computer vision approaches, such as manual feature extraction or rule-based gesture recognition, struggle to handle the complexity and variability of hand movements, lighting conditions, and signer differences.\n",
    "\n",
    "Deep learning offers a robust solution to this challenge. Convolutional Neural Networks (CNNs) can automatically learn spatial features from ASL hand gestures, enabling high-accuracy recognition of hand signs representing individual letters. When combined with Large Language Models (LLMs), which excel at contextual understanding and sequence reconstruction, the system can convert recognized letter sequences into coherent text output. \n",
    "\n",
    "This integration leverages CNNs for visual understanding and LLMs for linguistic composition, making deep learning an effective and scalable approach for ASL video-to-text translation.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Objectives\n",
    "\n",
    "The main objective of this study is to develop a deep learning-based system for translating ASL alphabet hand signs from video input into textual representation. Specifically, the study aims to:\n",
    "\n",
    "1. **Implement a CNN-based Model**  \n",
    "   To accurately recognize ASL hand signs representing individual letters from video frames.\n",
    "\n",
    "2. **Integrate a Large Language Model (LLM)**  \n",
    "   To reconstruct recognized letter sequences into meaningful text.\n",
    "\n",
    "3. **Evaluate the Modelâ€™s Performance**  \n",
    "   In terms of classification accuracy, robustness under varying lighting and motion conditions, and translation coherence.\n",
    "\n",
    "4. **Compare results across different CNN architectures**  \n",
    "   (e.g., VGG, ResNet, MobileNet) to identify the most effective model for hand sign recognition.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
