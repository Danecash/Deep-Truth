{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd4c54b6",
   "metadata": {},
   "source": [
    "# Data Collection and Preprocessing\n",
    "\n",
    "**Authors:** Usher Raymond Abainza, Dane Casey Casino, Kein Jake Culanggo, and Karylle dela Cruz\n",
    "\n",
    "---\n",
    "## 1 Data Collection\n",
    "\n",
    "**Image Dataset.** The dataset used in this study is the American Sign Language Letters dataset, an object detection dataset containing images of each ASL letter annotated with bounding boxes. Curated by David Lee, a data scientist specializing in accessibility, the dataset is publicly available and designed to support the development of computer vision models capable of interpreting ASL letters. Hosted on the Roboflow platform, the dataset allows for easy integration into YOLOv8-compatible pipelines and supports additional preprocessing and augmentation to improve model generalization. Each image contains one or more instances of ASL letters labeled with bounding boxes that define the spatial location of each sign, enabling models to learn both classification and localization. The dataset’s structure and flexibility make it particularly suitable for educational platforms, as it allows the creation of models that can accurately detect letters across varied hand positions, orientations, and backgrounds, bridging the gap between static recognition and real-world application scenarios.\n",
    "\n",
    "\n",
    "![image](./images/5.png)\n",
    "\n",
    "**Fig. 1.** Random sample images from the American Sign Language (ASL) Dataset with their corresponding labels\n",
    "\n",
    "**Video Dataset.** In addition to the static image dataset, we will create a custom video dataset to evaluate the system’s ability to recognize continuous ASL fingerspelling sequences. Each video will be recorded in 16:9 landscape orientation to maintain uniformity across all participants and recordings, with a plain white background, optimal and consistent lighting, and a fixed camera position. The framing will focus exclusively on the hand performing gestures, excluding the arm and other body parts, ensuring that only relevant gesture information is captured. Four researchers will participate, each producing short videos (approximately 5–10 seconds) spelling out 10-word sequences in ASL letters. Altogether, the dataset is expected to yield around 40 videos, covering all letters in the ASL alphabet for comprehensive testing.\n",
    "\n",
    "These controlled recording conditions are guided by prior research demonstrating the importance of environmental factors in gesture recognition. Hand gesture recognition systems are highly sensitive to illumination, and inconsistent lighting can substantially reduce accuracy. By ensuring uniform and optimal lighting, our recordings mitigate this source of error, as highlighted in studies emphasizing the impact of diverse illumination conditions on hand recognition performance [12].\n",
    "\n",
    "Similarly, background complexity is a known challenge in vision-based gesture recognition. Cluttered or patterned backgrounds can interfere with hand detection and classification, increasing misclassification rates. Using a plain white background addresses this challenge and ensures that model errors primarily reflect recognition capability rather than environmental distractions [13].\n",
    "\n",
    "The exclusion of extraneous body parts and focusing solely on the hand aligns with findings that occlusion and irrelevant visual features can reduce the reliability of gesture detection models. By limiting the frame to the hand itself, our dataset minimizes potential occlusion and ensures that feature extraction is concentrated on the relevant gestures [14].\n",
    "\n",
    "Finally, stable camera positioning contributes to maintaining uniformity in frame composition, reducing errors that arise from unintended rotation, scaling, or shifting of the hand in the frame. Together, these carefully controlled settings enhance the efficiency, robustness, and repeatability of the testing process, allowing reliable evaluation of sequence reconstruction and overall system performance [15].\n",
    "\n",
    "To clarify the role of the video dataset within the overall study, it was designed with two primary objectives:\n",
    "\n",
    "- **Assess real-world applicability:** Evaluate the system’s capacity to interpret continuous fingerspelling sequences rather than isolated static images.\n",
    "\n",
    "- **Examine temporal consistency:** Measure how accurately the model maintains recognition performance across varied hand motions, speeds, and transitions between gestures.\n",
    "\n",
    "\n",
    "By integrating the static image dataset with this custom video dataset, the study establishes a more comprehensive evaluation framework. This combination ensures that the system is tested not only for per-frame classification accuracy but also for its ability to reconstruct coherent letter sequences in dynamic, realistic settings, thereby supporting a robust assessment of both recognition and sequence reconstruction capabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a6ca8d",
   "metadata": {},
   "source": [
    "\n",
    "## 2 Data Preprocessing\n",
    "\n",
    "### 2.1 Image Dataset Preparation for Model Training and Testing\n",
    "\n",
    "To enhance the robustness and generalization capability of the model, extensive data augmentation was applied to the training set. For each original image in the training set, three augmented versions were generated, effectively quadrupling the size of the training data. This augmentation strategy exposes the model to diverse conditions and viewing angles that it may encounter in real-world scenarios, reducing overfitting and improving the model’s ability to generalize across unseen samples.\n",
    "\n",
    "The following augmentation techniques were randomly applied to training images:\n",
    "\n",
    "**Geometric Transformations:**\n",
    "\n",
    "- **Horizontal Flip:** Images were randomly flipped horizontally to introduce mirror variations of hand positions, helping the model recognize letters regardless of dominant hand orientation.\n",
    "\n",
    "\n",
    "- **Random Crop:** Applied with 0% minimum zoom and 20% maximum zoom to simulate variations in camera distance and framing, ensuring the model can handle different spatial contexts within the frame.\n",
    "\n",
    "\n",
    "- **Rotation:** Random rotation between -5° and +5° was applied to account for minor deviations in hand orientation during recording, reflecting realistic signing conditions.\n",
    "\n",
    "\n",
    "- **Shear:** Horizontal and vertical shear transformations of ±5° were introduced to simulate perspective distortions that may occur when the camera is not perfectly aligned.\n",
    "\n",
    "**Appearance Modifications:**\n",
    "\n",
    "- **Grayscale Conversion:** Applied to 10% of images to reduce the model’s reliance on color information, improving generalization across different skin tones and lighting conditions.\n",
    "\n",
    "- **Brightness Adjustment:** Random brightness variations between -25% and +25% simulate different lighting environments, ensuring the model remains accurate under low-light or overexposed conditions.\n",
    "\n",
    "- **Blur:** Gaussian blur up to 1.25 pixels accounts for motion blur and slight camera focus variations, mimicking natural movement during signing.\n",
    "\n",
    "\n",
    "These augmentation parameters were carefully selected to maintain the recognizability of ASL letters while introducing realistic variations. By combining geometric and appearance-based augmentations, the trained YOLOv8 models (YOLOv8n and YOLOv8s) are better equipped to generalize across diverse real-world conditions, including fluctuating lighting, camera angles, hand orientations, and environmental contexts. This ensures that the system remains reliable and accurate when applied to both controlled and dynamic video datasets.\n",
    "\n",
    "2.2   **Video Dataset Preparation for Testing**\n",
    "\n",
    "To evaluate the trained YOLOv8 models on realistic ASL fingerspelling sequences, a preprocessing pipeline was developed to handle video input and prepare frames for inference. Video data collected from mobile devices and cameras often contains orientation metadata and other inconsistencies that, if unaddressed, can reduce detection accuracy. This pipeline ensures that each frame is correctly oriented and maintains its original quality, allowing the model to perform reliably under varied recording conditions.\n",
    "\n",
    "**2.2.1 Video Input Handling.** Video files are loaded using OpenCV's VideoCapture interface, which extracts essential video properties including frame dimensions (width and height), frame rate (frames per second), and metadata tags. Maintaining these original properties ensures the model receives high-quality inputs and avoids distortions or unintended scaling that could affect hand detection and classification.\n",
    "\n",
    "**2.2.2 Rotation Correction.** Mobile devices often store video rotation as metadata rather than physically rotating the pixels, causing frames to appear misaligned when read by standard libraries. Misoriented frames can significantly reduce gesture recognition performance because YOLO-based models are sensitive to spatial orientation. To address this, the preprocessing pipeline automatically corrects frame orientation through the following steps:\n",
    "\n",
    "**2.2.3 Metadata Extraction.** \n",
    "The video’s rotation metadata is read using OpenCV's CAP_PROP_ORIENTATION_META property, which indicates the intended viewing orientation (0°, 90°, 180°, or 270°). This ensures that each frame is interpreted according to the device’s intended orientation.\n",
    "\n",
    "**Frame Rotation.** Based on the extracted rotation value, frames are rotated appropriately before inference:\n",
    "\n",
    "- 90° rotation: rotated 90° clockwise (cv2.ROTATE_90_CLOCKWISE)\n",
    "\n",
    "- 180° rotation: rotated 180° (cv2.ROTATE_180)\n",
    "\n",
    "- 270° rotation: rotated 90° counterclockwise (cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "\n",
    "- 0° or no metadata: frames are processed without rotation\n",
    "\n",
    "This step guarantees that hand gestures are consistently aligned across frames, reducing errors caused by unintentional rotation or orientation mismatches.\n",
    "\n",
    "By maintaining consistent orientation, preserving video quality, and standardizing frame dimensions, the preprocessing pipeline ensures that the YOLOv8 models can focus on learning hand features rather than compensating for recording inconsistencies. Combined with the augmented image dataset, this video preprocessing strategy contributes to robust model evaluation on continuous ASL fingerspelling sequences.\n",
    "\n",
    "```python\n",
    "def rotate_frame(frame, rotation):\n",
    "    \"\"\"Rotate frame based on rotation angle.\"\"\"\n",
    "    if rotation == 90:\n",
    "        return cv2.rotate(frame, cv2.ROTATE_90_CLOCKWISE)\n",
    "    elif rotation == 180:\n",
    "        return cv2.rotate(frame, cv2.ROTATE_180)\n",
    "    elif rotation == 270:\n",
    "        return cv2.rotate(frame, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "    else:\n",
    "        return frame \n",
    "```\n",
    "\n",
    "**2.2.4 Dimension Adjustment.** For videos rotated by 90° or 270°, the output video dimensions are automatically swapped (width ↔ height) to preserve the correct aspect ratio. This ensures that the rotated frames are displayed accurately without distortion, maintaining the spatial integrity of hand gestures for model inference.\n",
    "\n",
    "**2.2.5 Processing Pipeline.** The complete video preprocessing and inference pipeline operates as follows:\n",
    "\n",
    "- **Video Loading.** The input video file is opened, and metadata is extracted to determine properties such as frame dimensions, frame rate, and rotation information.\n",
    "\n",
    "\n",
    "- **Rotation Detection.** Orientation metadata is analyzed to identify the required rotation correction.\n",
    "\n",
    "\n",
    "- **Output Configuration.** The output video writer is initialized with corrected frame dimensions and the original frame rate, ensuring consistency between input and output videos.\n",
    "\n",
    "\n",
    "- **Frame-by-Frame Processing.** Each frame is sequentially:\n",
    "\n",
    "\n",
    "    - Read from the input video\n",
    "\n",
    "\n",
    "    - Rotated according to the metadata if necessary\n",
    "\n",
    "\n",
    "    - Passed to the YOLOv8 model for inference\n",
    "\n",
    "\n",
    "    - Annotated with detection results\n",
    "\n",
    "\n",
    "    - Written to the output video file\n",
    "\n",
    "\n",
    "- **Resource Cleanup.** Video capture and writer objects are properly released to prevent memory leaks and ensure system stability.\n",
    "\n",
    "This preprocessing pipeline guarantees that the YOLOv8 models receive consistently oriented frames with accurate dimensions and frame rates, allowing reliable recognition of ASL letters regardless of the original capture conditions. By automating rotation correction, aspect ratio adjustment, and frame processing, the pipeline reduces errors caused by device-specific recording inconsistencies and ensures that performance metrics reflect the model’s actual recognition capability rather than environmental artifacts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd06195",
   "metadata": {},
   "source": [
    "## 3   References\n",
    "\n",
    "[12] Haroon, M., Altaf, S., Ahmad, S., Zaindin, M., Huda, S., & Iqbal, S. (2022). Hand Gesture Recognition with Symmetric Pattern under Diverse Illuminated Conditions Using Artificial Neural Network. Symmetry, 14(10), 2045. https://doi.org/10.3390/sym14102045\n",
    "\n",
    "[13] Chen, R., & Tian, X. (2023). Gesture Detection and Recognition Based on Object Detection in Complex Background. Applied Sciences, 13(7), 4480. https://doi.org/10.3390/app13074480\n",
    "\n",
    "[14] Yasen, M., & Jusoh, S. (2019). A systematic review on hand gesture recognition techniques, challenges and applications. PeerJ Computer Science, 5, e218. https://doi.org/10.7717/peerj-cs.218\n",
    "\n",
    "[15] Zhang, T., Lin, H., Ju, Z., et al. (2020). Hand Gesture Recognition in Complex Background Based on Convolutional Pose Machine and Fuzzy Gaussian Mixture Models. Int. J. Fuzzy Syst., 22, 1330–1341. https://doi.org/10.1007/s40815-020-00825-w\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
